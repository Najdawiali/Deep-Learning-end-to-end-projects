# -*- coding: utf-8 -*-
"""DAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CytO4WnNtQKRwfcPWPyT-7ZZ3hLe5G1-
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import scipy.stats as stats

df = pd.read_excel('/content/بيانات سد الوحدة.xlsx')

## read the data
data = pd.read_excel('/content/بيانات سد الوحدة.xlsx')

data.shape

data.head()

# Step 1: Calculate Z-scores for each feature
z_scores = stats.zscore(data[['TOTAL_INFLOW', 'TOTAL_OUTFLOW', 'DAM_DAILY_LEVEL']])

# Identify outliers based on Z-scores (threshold set to 4)
outliers = (abs(z_scores) > 4).any(axis=1)

print(f"Number of outliers: {len(data[outliers])}")

# Step 4: Define a function to replace outliers with the previous valid value
def replace_outliers_with_previous_valid(data, outliers, columns):
    for col in columns:
        for idx in range(1, len(data)):
            if outliers[idx]:
                # Look back until a valid (non-outlier) value is found
                look_back_idx = idx
                while look_back_idx > 0 and outliers[look_back_idx]:
                    look_back_idx -= 1

                # If we find a valid previous value, replace the outlier
                if look_back_idx > 0:
                    data[col].iloc[idx] = data[col].iloc[look_back_idx]
    return data
# Apply the function to handle outliers in the 'TOTAL_INFLOW', 'TOTAL_OUTFLOW', and 'DAM_DAILY_LEVEL' columns
data = replace_outliers_with_previous_valid(data, outliers, ['TOTAL_INFLOW', 'TOTAL_OUTFLOW', 'DAM_DAILY_LEVEL'])

print("\nData after replacing outliers with the previous valid value:")
print(data)



## ensure DAM_DAILY_DATE is in datetime format, and not a string
data['DAM_DAILY_DATE'] = pd.to_datetime(data['DAM_DAILY_DATE'])

print(data.isnull().sum())

## Set DAM_DAILY_DATE as an index, so it can be easier to filter rows
data.set_index('DAM_DAILY_DATE', inplace=True)

# Features and target columns
features = ['DAM_DAILY_LEVEL', 'TOTAL_VOLUME', 'STORAGE_PERCENTAGE', 'MAX_CAPACITY']
targets = ['TOTAL_INFLOW', 'TOTAL_OUTFLOW']

# Normalize data, it's needed to ensure all of the data are valued between same range, since Neural Network is senstive to the the magnitude of data.
# The data is scaled with a value between 0 and 1
scaler = MinMaxScaler()

# Create an empty DataFrame to store the scaled values
data_scaled = pd.DataFrame(index=data.index)

# Apply MinMaxScaler independently to each column in features and targets
for column in features + targets:
    # Scale each column separately
    data_scaled[column] = scaler.fit_transform(data[[column]])

print(data[features + targets].max())

print(scaler.scale_)

#The function takes a dataset and creates sequences of a specified length (sequence_length).
# Each sequence contains a subset of the dataset for the given feature_columns as input (X) and the corresponding values for the target_columns as output (y)
def create_sequences(data, feature_columns, target_columns, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data.iloc[i:i + sequence_length][feature_columns].values)
        y.append(data.iloc[i + sequence_length][target_columns].values)
    return np.array(X), np.array(y)

# Set sequence length and create sequences
sequence_length = 30 # predict on 30 days period
X, y = create_sequences(data_scaled, features, targets, sequence_length)

# Split data into training (2010-2018) and validation (2019)
train_index = data.loc['2010':'2018'].shape[0] - sequence_length
X_train, X_val = X[:train_index], X[train_index:]
y_train, y_val = y[:train_index], y[train_index:]

# Build the LSTM model
# Sequential() is used to stack layers linearly, where each layer's output becomes the input for the next layer
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(sequence_length, len(features))),
    Dropout(0.3),
    LSTM(128, return_sequences=True),
    Dropout(0.3),
    LSTM(128),
    Dense(64, activation='relu'),
    Dense(len(targets))
])

model.compile(optimizer='adam', loss='mse')

# Train the model
# stop training when the model's performance on the validation set no longer improves
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[es],
    verbose=1
)

# forecast_future function predicts a specified number of future time steps by iteratively using the last sequence of features and the model's predictions to forecast the next values
def forecast_future_with_noise(data, model, sequence_length, feature_columns, steps, noise_scale=0.01):
    predictions = []
    # Ensure input_sequence is the correct shape
    input_sequence = data.iloc[-sequence_length:][feature_columns].values
    for step in range(steps):
        input_sequence = input_sequence[-sequence_length:]
        if input_sequence.shape[0] < sequence_length:
            pad_size = sequence_length - input_sequence.shape[0]
            input_sequence = np.vstack([np.zeros((pad_size, len(feature_columns))), input_sequence])
        # Add batch dimension and predict
        input_sequence_expanded = np.expand_dims(input_sequence, axis=0)
        pred = model.predict(input_sequence_expanded, verbose=0)[0]

        # Add noise to predictions
        pred += np.random.normal(scale=noise_scale, size=pred.shape)

        predictions.append(pred)

        # Update input_sequence
        new_row = np.zeros((1, len(feature_columns) + len(pred)))
        new_row[0, :len(feature_columns)] = input_sequence[-1]  # Last feature values
        new_row[0, len(feature_columns):] = pred  # Predicted targets with noise
        input_sequence = np.vstack([input_sequence, new_row[0, :len(feature_columns)]])
    return np.array(predictions)


# Forecast 366 days for 2020
forecast_steps = 366
future_predictions = forecast_future_with_noise(data_scaled, model, sequence_length, features, forecast_steps)

# Zero Padding: Adds zeros to the feature columns (which are not predicted).
# Combine with Predictions: Stacks the padded feature columns with the predicted target values.
# Inverse Scaling: Rescales the combined data back to the original scale, and then selects the target columns.


zero_padding = np.zeros((future_predictions.shape[0], len(features)))
pred_with_padding = np.hstack([zero_padding, future_predictions])
future_predictions_rescaled = scaler.inverse_transform(pred_with_padding)[:, -len(targets):]

# Save predictions to DataFrame
dates_2020 = pd.date_range(start='2020-01-01', end='2020-12-31')
predictions_df = pd.DataFrame(
    future_predictions_rescaled,
    index=dates_2020,
    columns=targets
)

# Save predictions to CSV
predictions_df.to_csv('predictions_2020.csv')

print("Predictions for 2020:")
print(predictions_df)

# Evaluate model performance

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Get the true values for 2019 (you already know this, it's in the data)
y_true = y_val  # These are the actual values from 2019 (from the validation set)

# Predict the values for 2019 using the trained model
y_pred = model.predict(X_val)

# Calculate MAE, MSE, and RMSE
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)


# A lower MAE indicates better prediction accuracy.
# A lower MSE indicates better model performance.
# A lower RMSE indicates better performance.

# Print the results
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")